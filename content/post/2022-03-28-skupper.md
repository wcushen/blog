---
layout:     post
title:      "Deep Dive with Skupper"
subtitle:   ""
description: "Skupper Rocks"
excerpt: "Skupper rocks again"
date:       2022-03-27
author:         "Will Cushen"
image: "/img/2018-04-11-service-mesh-vs-api-gateway/background.jpg"
published: true
tags:
    - Microservice
categories: [ Tech ]
URL: "/2022-skupper"
---

## What is Skupper

Skupper is open-source project on the rise in the Kubernetes ecosystem. It's marketed as a layer 7 service interconnect that allows namespaces-workloads across clusters to connect as if there were local. 

The topopoly is relatively straightforward with the lightweight AMPq Skuppr Router brokering the traffic between namespaces. 


### Removing Silos for Hybrid Cloud

Understanding the need the Skupper means taking stock of the growing hybrid-cloud boom as many organisations gripe with the need to connect on-premise, private and public cloud applications. The inter-connect possiblities available through Skupper however, are not limited to simply two or more Kubernetes clusters - virtualized, even mainframe-based workloads can be included in Skupper's fabric.

This post however, our use case will be confined to setup connectivity and encryption (over mTLS) between two OpenShift 4.10 clusters.

## About the deployment

We will create a front and back-end microservice, deploying each in a separate cluster to show the 'magic' of Skupper. The example used can be found in these Kubernetes docs here (https://kubernetes.io/docs/tasks/access-application-cluster/connecting-frontend-backend/) where we'll apply some minor tweaking to cater for some OpenShift and Skupper nuances, which we will outline below. 

### Step 1: Installing Skupper CLI

First things first, we will need to grab the latest version of the Skupper CLI from https://github.com/skupperproject/skupper-cli/releases. Let's add it to our path and make it executable.

```
$ curl https://skupper.io/install.sh | sh

$ sudo mv .local/bin/skupper /usr/local/bin 
```

### Step 2: Create our namespaces

Log into your OCP clusters with `oc login`. We have created a `north` and a `south` namespace, which we will be dpeploying our two applications into. 

We can see the output of `skupper status` proves the absence of anything Skupper at this point.

```
$ oc project
Using project "north" on server "https://api.cluster-one.example.com:6443".

$ skupper status
Skupper is enabled for namespace "north" in interior mode. It is connected to 1 other site. It has 1 exposed service.
The site console url is:  https://skupper-north.apps.cluster-one.example.com
The credentials for internal console-auth mode are held in secret: 'skupper-console-users'
``` 

```
$ oc project
Using project "south" on server "https://api.cluster-two.example.com:6443".

$ skupper status
Skupper is enabled for namespace "south" in interior mode. It is connected to 1 other site. It has 1 exposed service.
The site console url is:  https://skupper-south.apps.cluster-two.example.com
The credentials for internal console-auth mode are held in secret: 'skupper-console-users'
```

### Step 3: Installing Skupper Router and Controller

Next we will roll out the Skupper componentry being the Router and Controller in each namespace, responsible for creating our Virtual Application Network (VAN) and watching for service annotations (`internal.skupper.io/controlled: "true"`), respectively.  

For redundancy, we can also up the replica count on the router via `--router` to specify two or greater.

```
$ skupper init --site-name north
Skupper is now installed in namespace 'north'.  Use 'skupper status' to get more information.

$ oc get pod
NAME                                          READY   STATUS    RESTARTS   AGE
skupper-router-555dcb8f4d-5g24z               2/2     Running   0          6s
skupper-service-controller-547d68b9ff-24bfq   1/1     Running   0          4s
```

```
$ skupper init --site-name south
Skupper is now installed in namespace 'south'.  Use 'skupper status' to get more information.

$ oc get pod
NAME                                          READY   STATUS    RESTARTS   AGE
skupper-router-7f6d6dfb5f-ct7nd               2/2     Running   0          7s
skupper-service-controller-85f76cfdb8-jlgh6   1/1     Running   0          5s
```

### Step 4: Connecting our namespaces

Let's move onto generating a link token on the namespace that is serving as the **Listener** (`north`) in this instance.

```
$ skupper token create $HOME/secret.yaml
Token written to /home/lab-user/secret.yaml 
```

This renders a YAML file for the **Connecting** namespace on the `south` cluster to maintain as a `secret`.

Once transferred over via `scp` or other means, we can then create that link to instantiate the linkage between the two clusters.

```
$ skupper link create $HOME/secret.yaml
Site configured to link to https://claims-north.apps.cluster-one.example.com:443/dfd9ec0d-af2d-11ec-b421-0a126f52272a (name=link1)
Check the status of the link using 'skupper link status'.
```

It's important to know that once setup, in a Skupper mesh of **more** than two clusters; traffic will be completely bi-directional meaning that if the originating namespace (where the token was created initially) goes down; the network between those remaining participating clusters continues to be active.  

```
$ skupper link status
Link link1 is active
```

### Step 5: Deploy our two discrete applications

Now with our multi-cluster Skupper network in place, it's time to spin up the front and back-end services that will comprise our microservice using the YAML defintions verbatim from the documentation link above. 

NOTE: Going back to those 'tweaks' we said we would make; we need to:

1. Allow the `default` Service Accounts in both namespaces to use the privileged port of **80** via `oc adm policy add-scc-to-user privileged -z default`

2. Update the name of the backend deployment to `hello`, since this will be the internal DNS name by which the frontend sends requests to the backend worker Pods (set inside `nginx.conf`). Unfortunately we can't as of yet modify the Service name of a Deployment when we execute a `skupper expose`

Let's create the backend first in the `south` namespace.

```
$ wget -O- -q https://k8s.io/examples/service/access/backend-deployment.yaml | sed  "s/name: backend/name: hello/g" | oc apply -f -
deployment.apps/hello created
```

Next the frontend deployment in the `north` namespace.

```
$ oc apply -f https://k8s.io/examples/service/access/frontend-deployment.yaml
deployment.apps/frontend created
```

Now create and expose the frontend service.

At this point, we can now initiate the frontend service and expose as a typical OpenShift Router for external access. Alternatively, we could expose via `--type LoadBalancer` and acces via an `externalIP`.

```
$ oc expose deployment frontend --port 8080 && oc expose service frontend
```

### Step 6: Observe that the application does not work

Remembering that it won't yet have a connected backend, nor will the backend have a public ingress. 

curl

### Step 7: Expose the backend

The `skupper expose deployment` command will invoke the creation of a standard K8s Service with the addition of the necesary selectors so that traffic is brokered through the Skupper router. 

```
$ skupper expose deployment/hello --port 8080
```

Observe the Service created.

```
$ oc describe svc hello-world-backend
...
Annotations:       internal.skupper.io/controlled: true
Selector:          application=skupper-router,skupper.io/component=router
```

### Step 8: Validate the frontend now responds

Now we're at a point where we can expect something back from the frontend with the backend now exposed via Skupper.

You should see the message generated by the backend:

```
{"message":"Hello"}
```

## Conclusion

Skupper as you can see, has been architected so each application administrator is in control of the their own inter-cluster connectivity as opposed to a single `cluster-admin`. Some may argue the drawback as a result, is a decentalised mesh that could become problematic at scale when it comes to management and observability from a single point. Ultimately, it really depends in your environment who you intend to give the keys to. 

Finally, if we wanted to observe a graphical view of the sites and exposed services, the default-enabled Skupper Console provides us with some useful network topology data which we didn't look at in this demo. Skupper's development can be tracked on the official site or directly from the code on the GitHub project. 