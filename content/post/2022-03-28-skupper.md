---
layout:     post
title:      "Deep Dive with Skupper"
subtitle:   ""
description: "Skupper Rocks"
excerpt: "Skupper rocks again"
date:       2022-03-27
author:         "Will Cushen"
image: "/img/2018-04-11-service-mesh-vs-api-gateway/background.jpg"
published: true
tags:
    - Microservice
categories: [ Tech ]
URL: "/2022-skupper"
---

## What is Skupper

Skupper is open-source project on the rise in the Kubernetees ecosystem. It's marketed as a layer 7 service interconnect (although condensed to HTTP and gRPC protocols) that allows namesapcd-worklaods across clusters to connect as if locally.

The topopoly is relatively straightforward with the lightweight AMPq Skuppr Router brokering the traffic between namespaces. 


## Removing Silos for Hybrid Cloud

Understanding the need the Skupper means taking stock of the grwoing hybrid-cloud boom as several enterprises gripe with the intention to connect on-prmsie, private and public cloud resources. 

The connecitvity methods owever are nt limtied ot inter Kubeertes cluste with Skupper we're able to extend the fabirc to what would often be classed as "legacy" apps namely virtualzied, even mainframe-based worklaods. 

This post however, we'll confine our use case to setup conneviiy and encryption (over mTLS) between two OpenShift 4.10 clusters.

## Steps

We will create a frontend and a backend microsevrice, deploying each in a disparate cluster show the magic of Skupper. The example used can be found in these docs here (https://kubernetes.io/docs/tasks/access-application-cluster/connecting-frontend-backend/) which some minor tweaking to cater for some OpenShift and Skupper nuances which we will outline below. 

First things first, we will need to the latest version of the skupper CLI from https://github.com/skupperproject/skupper-cli/releases. Let's add it to our path and make it executable.

``
$ curl https://skupper.io/install.sh | sh

$ sudo mv .local/bin/skupper /usr/local/bin 
``

2. Log into your OCP clusters with oc. We have created a `north` and a `south` namespace which we will be dpeploying our wokrlaods into. 

We can see that a `skupper status` highlghts the absence of anything Skupper at this point.

```
[lab-user@bastion ~]$ oc project
Using project "north" on server "https://api.cluster-6jqqg.6jqqg.sandbox1385.opentlc.com:6443".

[lab-user@bastion ~]$ skupper status
Skupper is enabled for namespace "north" in interior mode. It is connected to 1 other site. It has 1 exposed service.
The site console url is:  https://skupper-north.apps.cluster-6jqqg.6jqqg.sandbox1385.opentlc.com
The credentials for internal console-auth mode are held in secret: 'skupper-console-users'
``` 

```
[lab-user@bastion ~]$ oc project
Using project "south" on server "https://api.cluster-b9pn6.b9pn6.sandbox1044.opentlc.com:6443".

[lab-user@bastion ~]$ skupper status
Skupper is enabled for namespace "south" in interior mode. It is connected to 1 other site. It has 1 exposed service.
The site console url is:  https://skupper-south.apps.cluster-b9pn6.b9pn6.sandbox1044.opentlc.com
The credentials for internal console-auth mode are held in secret: 'skupper-console-users'
```

3. Next we will roll out the Skupper componetny of the Router and Controller in each namespace, responsibile for creating our VAN and watching for service annotations (`internal.skupper.io/controlled: "true"`), respectively.  

For reduncnacy, we can also iup the rplicas count on the routers via `--router` to specify two or greater

```
[lab-user@bastion ~]$ skupper init --site-name north
Skupper is now installed in namespace 'north'.  Use 'skupper status' to get more information.
[lab-user@bastion ~]$ oc get pod
NAME                                          READY   STATUS    RESTARTS   AGE
skupper-router-555dcb8f4d-5g24z               2/2     Running   0          6s
skupper-service-controller-547d68b9ff-24bfq   1/1     Running   0          4s

[lab-user@bastion ~]$ skupper init --site-name south
Skupper is now installed in namespace 'south'.  Use 'skupper status' to get more information.

[lab-user@bastion ~]$ oc get pod
NAME                                          READY   STATUS    RESTARTS   AGE
skupper-router-7f6d6dfb5f-ct7nd               2/2     Running   0          7s
skupper-service-controller-85f76cfdb8-jlgh6   1/1     Running   0          5s
```

3. We will move onto generating a link token on the namespace that is serving as the Listener (`north`) in this instance.

```
[lab-user@bastion ~]$ skupper token create $HOME/secret.yaml
Token written to /home/lab-user/secret.yaml 
```

This renders a YAML file for the Connecintg namespces on the adjoiing cluster to maintain as a secret

4. Once transferred over via scp or other means, we can then create that link to instantitiate the linkage between the two clusters

```
[lab-user@bastion ~]$ skupper link create $HOME/secret.yaml
Site configured to link to https://claims-north.apps.cluster-6jqqg.6jqqg.sandbox1385.opentlc.com:443/dfd9ec0d-af2d-11ec-b421-0a126f52272a (name=link1)
Check the status of the link using 'skupper link status'.
```

4. It's important to know that in a micorservice architeture comprises more than two clusters; traffic will be completely bi-direcitonal that if the originating namespace (where the token was create ditnially) goes down; the network between those remaining particpating clusterd will continue to be active.  

```
[lab-user@bastion ~]$ skupper link status
Link link1 is active
```

5. Now with our multi-cluster Skupper network in place, it's time to spin up the front and back-end services that will comprise our microservice using the YAML deinfitoins veratim in the Kuberneets Documetnation link above. 

NOTE: Citing those tweaks that we said we would make. 
1. We are going to allow the `default` Service Accounts in both namespaces to use the porivilaegd port of 80  and unread the the 
2. We are going to update the name of the backend deployment to `hello`, since this will be the internal DNS name by which the frontend sends requests to the backend worker Pods (set inside nginx.conf). Unfortunately with Skupper we can't as of yet moodiy the Service name of a Deployment when we execute a `skupper expose`

We will create the backend first in the `south` namespace

```
[lab-user@bastion ~]$ wget -O- -q https://k8s.io/examples/service/access/backend-deployment.yaml | sed  "s/name: backend/name: hello/g" | oc apply -f -
deployment.apps/hello created
```

Next the frontend deployment in the `north` namespace

```
[lab-user@bastion ~]$ oc apply -f https://k8s.io/examples/service/access/frontend-deployment.yaml
deployment.apps/frontend created
```

Create and expose the frontend service

At this point, we can now instantiate the frotnend service and expose as a usual route for external access. ALterntively, we could expose via `--type LoadBalancer` and acces via an `externalIP` 

oc expose deployment hello-world-frontend --port 8080 && oc expose service frontend

Remebering that it won't yet have a connected backend, nor will the backend have no public ingress. 

curl


Expose the backend


The `skupper expose deployment` command will invoke the creation of a standard K8s Service with the addition of the necesary selctros so that traffic is brokered through the skupper-router. 

skupper expose deployment/hello-world-backend --port 8080

[lab-user@bastion ~]$ oc describe svc hello-world-backend
...
Annotations:       internal.skupper.io/controlled: true
Selector:          application=skupper-router,skupper.io/component=router


Test the frontend

With the backend now exposed via Skupper, we can now see it made available to our publicly accessbile frontend route. 

You should see the message generated by the backend:

```
{"message":"Hello"}
```

Conclusion

This project is still under heavy development and improves every day. Skupper as you can see, has been architecterd so eahc application administer is control of the their own inter-cluster connecitvity as opposed to  single cluster-admin. Some may argue the drawback as a result, is a discetnralised Skupper 'mesh' could become problmeatic at scale when it comes to management and observiblity from a single point. - it really depedns in our environemtn who you intend to give the keys to. Skupper's development can be tracked on the official site or directly from the code on the GitHub project. Finally, if we wanted to observe a grpahical veiw of the sites and exposed services the default-enabled Skupper Conosle provides as with some useful network topolofy dtaa.  